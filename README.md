CONTENTS OF THIS FILE
---------------------

 * Introduction
 * Requirements

INTRODUCTION
------------

The MovieLens dataset from movielens.org contains 27,753,444 ratings and 1,108,997 tag applications across 58,098 movies. These data were created by 283,228 users between January 09, 1995 and September 26, 2018. Our objective is to use PySpark, the Python API for Apache Spark, to conduct an analysis of the dataset, allow for user input to interact with the data, and use a recommendation engine to provide user and content-based recommendations.

REQUIREMENTS
------------

This project requires setting up Apache Spark for the distributed computing and using PySpark, the python API, to manipulate the dataset. I use Jupyter Notebook to access PySpark and import relevant libraries such as ipywidgets for widgets, plotly and seaborn for visualizations, pandas for data manipulation, and SKLearn and Fuzzy Wuzzy for the recommendation engine. These can be installed using pip. 


FEATURES
------------
I have used SparkSession and its supported DataFrames to read and store the .csv files rather than RDDs out of personal preference because it allows for easier processing and my experience with Python and its supported Pandas library for data manipulation and analysis. This feature is indicative of Spark’s accessibility, whereby non-specialised data engineers are able to use a domain specific language API to manipulate our distributed data. 
	I structure my project by first summarising the dataset and exploring the information through visualizations using the Plotly open-source library. This provides users with an idea of what information we have in an intuitive and digestible manner. My visualizations summarise the dataset by movie genre, movie release year, movies by unique watches, and by rating. They also enable users to interact with the visualization through mouse hovers, zooming in and out, and screenshot capabilities. 
	I use the ipywidgets library available for Python to create widgets that users can interact with and filter a vast amount of information easily. This includes filtering for movies within a specific genre, movies released by year, the most or least watched genre, top ten movies with the highest and lowest ratings (average), top ten most or least watched movies, searching for a movie’s average rating and number of watchers by movie ID, and details of a user’s movie preferences including their favourite genre. 
  I continue to use a combination of visualizations and widgets to allow users to compare the movie preferences of two movie users from the dataset. I define preferences through various factors such as total number of movies watched from a specific genre, highest rated movies, movies watched from a specific year, and keywords used to describe movies watched. I combine these factors to determine movie preferences after reflecting upon how my own personal preference would be determined. For instance, while I am a big psycho-thriller and horror movie buff who prefers more recent releases, I do not hand out five star ratings easily. Moreover, I inexplicably despise watching anything that was released before I was born. Fortunately, the MovieLens Dataset provides sufficient information to determine such insights about the users as well. 
	After analysing the preferences of our two users, I use a scatter plot visualization to apply the same factors to the dataset as a whole  because a single picture is worth a thousand words, indeed! In the scatter plot, I group users by the majority genre watched and average rating and create an animation frame in my visualization to examine how these preferences change over the years. 
  My use of static and interactive visualizations using the Plotly and Seaborn libraries enable easier analysis of the dataset. In addition to my visualizations, I created an MVP of a user-based and content-based recommendation engine. For the purposes of our project, I used the smaller dataset provided ('ml-latest-small') since using the larger dataset was computationally expensive. I further divided the smaller dataset by 80% using sampling to speed up the computation time. When vectorised, this resulted in a 20,161 x 19 matrix which was easier to compute given the time and resource constraint. This  inevitably introduced a slight variation in the accuracy of the recommendation engine, as indicated by our analysis of User 27 in our demo and markdown file. 
	While there are various ways to create recommendation engines including user-to-user based filtering, item-to-item based filtering,  and collaborative filtering, I opted for content-based filtering, which primarily focuses on item similarity rather than drawing connections between different users of similar choices in movies because of the assumption that users are extremely subjective in their preferences. Preferences are often eclectic and vary by mood, circumstance, and context. The information in the dataset did not provide enough demographic and activity information  about users to accurately draw conclusions between user preferences but it provided sufficient information to at least conduct content-based filtering.
	I also opted to use Python Pandas to read and store the data into a dataframe so that I could leverage Python’s SKLearn library for Term Frequency-Inverse Document Frequency (TF-IDF) Vectorisation rather than using PySpark's HashingTF and/or CountVectorizer for feature transformation and extraction out of personal preference and familiarity with SKLearn. That being said, there is no significant difference in Spark’s implementation of TF-IDF, which may also be faster computationally in comparison to Pandas because of the distributed processing. 
	My recommendation engine can take either of two inputs - the first one is takes a User ID to determine movie preferences based on the User’s most watched genre. The secon second based on a movie title to determine closely related movies. For the former, I filter my TF-IDF matrix by genre and recommend the top ten titles most relevant within the genre. This relies solely on a user’s most preferred genre without accounting for other factors such as movie release year or rating because not all users have provided ratings for a movie or even the users that have provided ratings only provide a handful. For the latter, I use  the matrix in conjunction with cosine similarity to determine the distance score of a title compared to other titles in the matrix. I also use the Fuzzy Fuzzy library for string matching to compare the user input with the actual title in the dataset. This library uses the Levenshtein distance, a string metric to measure the difference between two sequences, to match two strings. The top ten titles most similar to the input title are produced. 
